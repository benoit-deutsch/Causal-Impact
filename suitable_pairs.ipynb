{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Automated City Pairing for Causal Analysis\n",
                "\n",
                "This notebook iterates through all available cities, identifies suitable control candidates using advanced metrics, and exports the findings to `city_pairings.csv`. This automates the selection process for any configuration of target and controls."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2296f1c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
                "from tqdm import tqdm\n",
                "\n",
                "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
                "warnings.simplefilter(action='ignore', category=UserWarning)\n",
                "\n",
                "# Constants from causal_impact.ipynb\n",
                "target_col = 'Barcelona' # Default example\n",
                "pre_beg, pre_end = '2023-01-01', '2023-05-31'\n",
                "t1_thresh = 0.8\n",
                "t2_thresh = 0.6"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8a617330",
            "metadata": {},
            "source": [
                "## 1. Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b5d5e32",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_long = pd.read_csv('sales_data.csv')\n",
                "df_long['Date'] = pd.to_datetime(df_long['Date'])\n",
                "df = df_long.pivot(index='Date', columns='City', values='Value')\n",
                "df.index.freq = 'D'\n",
                "\n",
                "cities = sorted(df.columns.tolist())\n",
                "print(f\"Processing {len(cities)} cities...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "938d8a02",
            "metadata": {},
            "source": [
                "## 2. Refined Selection Step (Sequential Stationarity)\n",
                "\n",
                "We evaluate each city. A city is selected if it has high correlation AND can be made stationary through our standard pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd5e7a51",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_stationary_transform(series, seasonal_period=7):\n",
                "    \"\"\"Returns (step_name, transform_func) or (None, None).\"\"\"\n",
                "    def is_stationary(s):\n",
                "        try:\n",
                "            return adfuller(s.dropna())[1] < 0.05\n",
                "        except:\n",
                "            return False\n",
                "\n",
                "    if is_stationary(series):\n",
                "        return \"Raw\", lambda s: s\n",
                "    \n",
                "    try:\n",
                "        s_log = np.log(series)\n",
                "        if is_stationary(s_log): return \"Log\", lambda s: np.log(s)\n",
                "        \n",
                "        s_diff = s_log.diff()\n",
                "        if is_stationary(s_diff): return \"Log+Diff\", lambda s: np.log(s).diff()\n",
                "        \n",
                "        s_seasonal = s_diff.diff(seasonal_period)\n",
                "        if is_stationary(s_seasonal): \n",
                "            return \"Log+Diff+Seasonal\", lambda s: np.log(s).diff().diff(seasonal_period)\n",
                "    except:\n",
                "        pass\n",
                "        \n",
                "    return None, None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d42dbf1f",
            "metadata": {},
            "source": [
                "## 3. Batch Evaluation (Adapting select_best_controls loop)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9d4dd7e",
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "df_pre = df.loc[pre_beg:pre_end]\n",
                "\n",
                "for target in tqdm(cities, desc=\"Targets\"):\n",
                "    for city in cities:\n",
                "        if target == city:\n",
                "            continue\n",
                "            \n",
                "        corr_raw = df_pre[target].corr(df_pre[city])\n",
                "        step_name, transform_func = get_stationary_transform(df_pre[city])\n",
                "        \n",
                "        corr_trans = None\n",
                "        granger_p = None\n",
                "        var_ratio = None\n",
                "        \n",
                "        if step_name:\n",
                "            s_city_trans = transform_func(df_pre[city]).dropna()\n",
                "            s_target_trans = transform_func(df_pre[target]).dropna()\n",
                "            \n",
                "            # Behavioral Correlation\n",
                "            joined = pd.concat([s_city_trans, s_target_trans], axis=1).dropna()\n",
                "            corr_trans = joined.iloc[:, 0].corr(joined.iloc[:, 1])\n",
                "            \n",
                "            # Granger Causality\n",
                "            try:\n",
                "                granger_result = grangercausalitytests(joined[[target, city]], maxlag=2, verbose=False)\n",
                "                granger_p = granger_result[1][0]['params_ftest'][1]\n",
                "            except: granger_p = 1.0\n",
                "            \n",
                "            # Variance Ratio (Volatility Matching)\n",
                "            var_ratio = s_city_trans.std() / s_target_trans.std()\n",
                "        \n",
                "        # Initial Assignment with Variance and Correlation Filter\n",
                "        tier = \"None\"\n",
                "        if corr_trans and corr_trans > t1_thresh:\n",
                "            if 0.5 < var_ratio < 2.0:\n",
                "                tier = \"Tier 1 (Strict)\"\n",
                "            else:\n",
                "                tier = \"Rejected (High Variance)\"\n",
                "        elif corr_trans and corr_trans > t2_thresh and corr_raw > t2_thresh:\n",
                "            tier = \"Tier 2 (Fallback)\"\n",
                "        \n",
                "        results.append({\n",
                "            'Target': target,\n",
                "            'City': city,\n",
                "            'Correlation_Raw': corr_raw,\n",
                "            'Correlation_Transformed': corr_trans,\n",
                "            'Granger_p_value': granger_p,\n",
                "            'Variance_Ratio': var_ratio,\n",
                "            'Selection_Tier': tier\n",
                "        })\n",
                "\n",
                "pairings_df = pd.DataFrame(results)\n",
                "print(f\"\\nPrepared {len(pairings_df)} pairings for further analysis.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c4d5e6f7",
            "metadata": {},
            "source": [
                "## 4. Power and Volume Calculator\n",
                "\n",
                "We estimate the **Minimum Detectable Effect (MDE)** to ensure the experiment is sufficiently powered. These metrics are added to all tiered pairings and exported to CSV."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d5e6f7g8",
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_mde_refined(target_series, control_series, post_days=14, confidence=0.95, power=0.8):\n",
                "    import statsmodels.api as sm\n",
                "    from scipy.stats import norm\n",
                "    \n",
                "    # 1. Critical values\n",
                "    z_alpha = norm.ppf(1 - (1 - confidence) / 2)\n",
                "    z_beta = norm.ppf(power)\n",
                "    z_combined = z_alpha + z_beta\n",
                "    \n",
                "    # 2. Get high-frequency residual noise (Stationary Noise)\n",
                "    y = target_series.diff().dropna()\n",
                "    x = control_series.diff().dropna()\n",
                "    \n",
                "    x_reg = sm.add_constant(x)\n",
                "    model = sm.OLS(y, x_reg).fit()\n",
                "    sigma_daily = np.sqrt(model.mse_resid)\n",
                "    \n",
                "    # 3. Detection Threshold for the TOTAL sum over post_days\n",
                "    total_incremental_units_required = z_combined * sigma_daily * np.sqrt(post_days)\n",
                "    \n",
                "    # Percentage MDE vs baseline total volume\n",
                "    baseline_total_volume = target_series.mean() * post_days\n",
                "    mde_pct = total_incremental_units_required / baseline_total_volume\n",
                "    \n",
                "    return mde_pct, total_incremental_units_required\n",
                "\n",
                "def augment_with_power(df_pairs, df_all, durations=[14, 30]):\n",
                "    \"\"\"Calculates power metrics for all entries with a valid selection tier.\"\"\"\n",
                "    augmented_rows = []\n",
                "    # We only calculate power for Tier 1 and Tier 2 to save time\n",
                "    valid_pairs = df_pairs[df_pairs['Selection_Tier'].str.contains(\"Tier\")].copy()\n",
                "    \n",
                "    for idx, row in tqdm(valid_pairs.iterrows(), total=len(valid_pairs), desc=\"Calculating Power\"):\n",
                "        target, city = row['Target'], row['City']\n",
                "        s_target = df_all[target].loc[pre_beg:pre_end]\n",
                "        s_city = df_all[city].loc[pre_beg:pre_end]\n",
                "        \n",
                "        res = row.to_dict()\n",
                "        for d in durations:\n",
                "            mde_p, mde_a = calculate_mde_refined(s_target, s_city, post_days=d)\n",
                "            res[f\"Volume_{d}d\"] = s_target.mean() * d\n",
                "            res[f\"MDE_{d}d (%)\"] = mde_p\n",
                "            res[f\"Impact_{d}d_Total\"] = mde_a\n",
                "        augmented_rows.append(res)\n",
                "    \n",
                "    return pd.DataFrame(augmented_rows)\n",
                "\n",
                "# Run Power Analysis\n",
                "augmented_pairings = augment_with_power(pairings_df, df)\n",
                "\n",
                "# Merge back with non-tiered results just for the CSV export completeness (optional, but requested)\n",
                "final_export_df = pd.concat([augmented_pairings, pairings_df[~pairings_df['Selection_Tier'].str.contains(\"Tier\")]], sort=False)\n",
                "final_export_df.to_csv('city_pairings.csv', index=False)\n",
                "print(f\"\\nSaved augmented data to city_pairings.csv\")\n",
                "\n",
                "# Preview the Top Rows for the notebook display\n",
                "display_df = augmented_pairings[augmented_pairings['Selection_Tier'] == 'Tier 1 (Strict)'].sort_values('Correlation_Transformed', ascending=False).head(10)\n",
                "\n",
                "# Formatting for Jupyter Display\n",
                "cols_v = [c for c in display_df.columns if \"Volume\" in c]\n",
                "cols_pct = [c for c in display_df.columns if \"(%)\" in c]\n",
                "cols_abs = [c for c in display_df.columns if \"Total\" in c]\n",
                "display_cols = [\"Target\", \"City\"] + cols_v + cols_pct + cols_abs\n",
                "\n",
                "format_dict = {col: \"{:.2%}\" for col in cols_pct}\n",
                "format_dict.update({col: \"{:.0f}\" for col in cols_abs})\n",
                "format_dict.update({col: \"{:.0f}\" for col in cols_v})\n",
                "\n",
                "display_df[display_cols].style.format(format_dict)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "glossary_cell",
            "metadata": {},
            "source": [
                "### Glossary & Interpretation Guide\n",
                "\n",
                "| Term | Definition |\n",
                "| :--- | :--- |\n",
                "| **Target** | The city where you plan to launch the campaign/intervention. |\n",
                "| **Volume_{d}d** | The total expected sales units in the target city over a {d}-day period. |\n",
                "| **Control** | The city acting as the benchmark. |\n",
                "| **MDE (%)** | **Minimum Detectable Effect**. The smallest percentage lift in total sales required over the period for statistical significance. |\n",
                "| **Impact_{d}d_Total** | **Absolute Detection Threshold**. The total number of *additional* units you need to sell across the **entire {d}-day window** to be sure the result is real. |\n",
                "\n",
                "### Example: Target = Barcelona | Control = City_18\n",
                "\n",
                "| Target | Control | Volume_14d | MDE_14d (%) | Impact_14d_Total | Volume_30d | MDE_30d (%) | Impact_30d_Total |\n",
                "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
                "| **Barcelona** | **City_18** | **35,000** | **12.85%** | **4,500** | **75,000** | **8.80%** | **6,600** |\n",
                "\n",
                "**How to read this:**\n",
                "\n",
                "1. **The 14-Day Scenario (2 Weeks)**\n",
                "- **Volume_14d (35,000)**: Over 14 days, Barcelona normally sells 35,000 units total.\n",
                "- **Impact_14d_Total (4,500)**: To be statistically significant, your campaign must generate **4,500 extra units** on top of the 35k.\n",
                "- **MDE_14d (12.85%)**: This means you need a **+12.85% lift** in total sales to \"break through\" the daily market noise.\n",
                "\n",
                "2. **The 30-Day Scenario (1 Month)**\n",
                "- **Volume_30d (75,000)**: Over 30 days, sales grow to 75,000 units.\n",
                "- **Impact_30d_Total (6,600)**: Running the test longer improves sensitivity. You only need **6,600 extra units** (not 9,000!) to reach significance.\n",
                "- **MDE_30d (8.80%)**: Your required lift dropped from ~13% to **under 9%**. The test is much more powerful if you wait longer."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}